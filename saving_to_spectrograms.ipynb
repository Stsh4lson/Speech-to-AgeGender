{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TRAIN_DATA_LEN: 3714569\nVAL_DATA_LEN: 1590319\n"
     ]
    }
   ],
   "source": [
    "import path_configs # noqa\n",
    "import tensorflow as tf\n",
    "import settings\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import numpy as np\n",
    "import librosa\n",
    "settings.init()\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=1.5, font=\"Arial\", style=\"white\")\n",
    "# from modules.ClassifierGenerators import (TrainClassifierGenerator,  # noqa\n",
    "#                                           ValidationClassifierGenerator)  # noqa\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assumes that array is not zero\n",
    "def scaled(tensor):\n",
    "    return (tensor-tf.math.reduce_min(tensor))/(tf.math.reduce_max(tensor)-tf.\n",
    "                                                math.reduce_min(tensor))\n",
    "def scaled_array(array):\n",
    "    return (array - np.min(array))/(np.max(array) - np.min(array))\n",
    "\n",
    "import scipy.spatial.distance as dist\n",
    "def similarity(x1, x2):\n",
    "    x1 = librosa.feature.mfcc(x1).flatten()\n",
    "    x2 = scipy.signal.resample(librosa.feature.mfcc(x2).flatten(), len(x1))\n",
    "    distance = dist.correlation(x1, x2)\n",
    "    return 1/(1+distance)\n",
    "\n",
    "from modules.DataPreprocessor import DataLoader\n",
    "dl = DataLoader()\n",
    "sample_rate = 48000\n",
    "window_time = dl.window_time\n",
    "frame_length = int(48000*window_time/1000)\n",
    "frame_step = frame_length//4\n",
    "def reverse_to_audio(db_spec):\n",
    "    audio_rev_spec = librosa.db_to_power(db_spec, ref=1.0)\n",
    "    print(\n",
    "        'reversing'\n",
    "    )\n",
    "    return librosa.feature.inverse.mel_to_audio(audio_rev_spec, sr=48000, n_fft=frame_length, hop_length=frame_length//4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# for set_name in ['test', 'train', 'val']:\n",
    "#     for gender_name in ['male', 'female']:\n",
    "#         for age_name in ['teens', 'twenties','seventies', 'fifties', 'fourties','thirties', 'sixties', 'eighties']:        \n",
    "#             Path(os.path.join('data', 'images',set_name, gender_name, age_name)).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# for spec_num in tqdm(np.arange(dl.train.shape[0]+1)):\n",
    "#     spec = dl.make_spectrogram(spec_num)\n",
    "#     paded_spec = dl.pad_spec(spec)\n",
    "#     set_type = dl.train.iloc[spec_num, 7]\n",
    "#     age = dl.train.iloc[spec_num, 2]\n",
    "#     gender = dl.train.iloc[spec_num, 3]\n",
    "#     for window in np.arange(0, paded_spec.shape[1]-64, 64):\n",
    "#         spec_window = paded_spec[:, window:window + 128]\n",
    "#         name = str('spec_' + str(window//64) + '_' + str(spec_num))\n",
    "#         np.save(os.path.join('data', 'images',set_type, gender, age, name), spec_window, allow_pickle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                               client_id  \\\n",
       "0      7eff9a54bdb0619deffda7609d5b8565278e3328de99e6...   \n",
       "1      3dbc57adea9742c6782b7c88b7ac313e4bb7e26374db2e...   \n",
       "2      5b1dd93d15c860f4d94c18ca33bb896b1b4a4f2f95f253...   \n",
       "3      70c21ca1cc3de05f3c23b11615cd7d5dd078e03cda0cd4...   \n",
       "4      db94fba2e15010f1a4aa114d910ac1e1a320c5dbd692a3...   \n",
       "...                                                  ...   \n",
       "38725  4ba8b91d601ee7f27be3011552ea94c8d076d9816d4a77...   \n",
       "38726  afcc38200b25a04019f5e3125535f1d93674ad6f0be9ac...   \n",
       "38727  e140a69135f35bc81e98140a267e18a382807d684099be...   \n",
       "38728  64bb74229c8a05aeeebe8aeb498d5a0389479625a0085d...   \n",
       "38729  ac0b5abbc3c48b594b7e837a23e68460b0b0329c4df554...   \n",
       "\n",
       "                               path        age  gender  \\\n",
       "0      common_voice_en_19956141.mp3    sixties  female   \n",
       "1      common_voice_en_20012566.mp3   fourties  female   \n",
       "2      common_voice_en_18343993.mp3      teens    male   \n",
       "3      common_voice_en_20791392.mp3    fifties  female   \n",
       "4      common_voice_en_20005534.mp3   fourties    male   \n",
       "...                             ...        ...     ...   \n",
       "38725  common_voice_en_19769331.mp3  seventies  female   \n",
       "38726    common_voice_en_193416.mp3    sixties  female   \n",
       "38727  common_voice_en_21169446.mp3   twenties    male   \n",
       "38728  common_voice_en_19761030.mp3   twenties    male   \n",
       "38729  common_voice_en_19683374.mp3   twenties  female   \n",
       "\n",
       "                                                sentence  length  length_cut  \\\n",
       "0      He had success right off the bat with Do You B...  266112      199680   \n",
       "1      While the Doctor aids Clent, Penley leaves the...  252288      233280   \n",
       "2                           The hyena proceeded to dine.  161280       84480   \n",
       "3      In Manchuria and Siberia, they mate during Jan...  271872      216960   \n",
       "4      The reconfigured centre would have been brande...  205056      149760   \n",
       "...                                                  ...     ...         ...   \n",
       "38725  The first volume was written during Ovid's jou...  288000      165120   \n",
       "38726             Play Lil Hardin Armstrong from Itunes.  253440      120960   \n",
       "38727               It is primarily a farming community.  196992       86400   \n",
       "38728                  The Ewings had thirteen children.  171648      168960   \n",
       "38729  Under protest but at Ben-Gurion's insistence, ...  357120      293760   \n",
       "\n",
       "      set_type  \n",
       "0        train  \n",
       "1        train  \n",
       "2        train  \n",
       "3        train  \n",
       "4        train  \n",
       "...        ...  \n",
       "38725    train  \n",
       "38726    train  \n",
       "38727      val  \n",
       "38728      val  \n",
       "38729    train  \n",
       "\n",
       "[38730 rows x 8 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>client_id</th>\n      <th>path</th>\n      <th>age</th>\n      <th>gender</th>\n      <th>sentence</th>\n      <th>length</th>\n      <th>length_cut</th>\n      <th>set_type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7eff9a54bdb0619deffda7609d5b8565278e3328de99e6...</td>\n      <td>common_voice_en_19956141.mp3</td>\n      <td>sixties</td>\n      <td>female</td>\n      <td>He had success right off the bat with Do You B...</td>\n      <td>266112</td>\n      <td>199680</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3dbc57adea9742c6782b7c88b7ac313e4bb7e26374db2e...</td>\n      <td>common_voice_en_20012566.mp3</td>\n      <td>fourties</td>\n      <td>female</td>\n      <td>While the Doctor aids Clent, Penley leaves the...</td>\n      <td>252288</td>\n      <td>233280</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5b1dd93d15c860f4d94c18ca33bb896b1b4a4f2f95f253...</td>\n      <td>common_voice_en_18343993.mp3</td>\n      <td>teens</td>\n      <td>male</td>\n      <td>The hyena proceeded to dine.</td>\n      <td>161280</td>\n      <td>84480</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>70c21ca1cc3de05f3c23b11615cd7d5dd078e03cda0cd4...</td>\n      <td>common_voice_en_20791392.mp3</td>\n      <td>fifties</td>\n      <td>female</td>\n      <td>In Manchuria and Siberia, they mate during Jan...</td>\n      <td>271872</td>\n      <td>216960</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>db94fba2e15010f1a4aa114d910ac1e1a320c5dbd692a3...</td>\n      <td>common_voice_en_20005534.mp3</td>\n      <td>fourties</td>\n      <td>male</td>\n      <td>The reconfigured centre would have been brande...</td>\n      <td>205056</td>\n      <td>149760</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>38725</th>\n      <td>4ba8b91d601ee7f27be3011552ea94c8d076d9816d4a77...</td>\n      <td>common_voice_en_19769331.mp3</td>\n      <td>seventies</td>\n      <td>female</td>\n      <td>The first volume was written during Ovid's jou...</td>\n      <td>288000</td>\n      <td>165120</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>38726</th>\n      <td>afcc38200b25a04019f5e3125535f1d93674ad6f0be9ac...</td>\n      <td>common_voice_en_193416.mp3</td>\n      <td>sixties</td>\n      <td>female</td>\n      <td>Play Lil Hardin Armstrong from Itunes.</td>\n      <td>253440</td>\n      <td>120960</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>38727</th>\n      <td>e140a69135f35bc81e98140a267e18a382807d684099be...</td>\n      <td>common_voice_en_21169446.mp3</td>\n      <td>twenties</td>\n      <td>male</td>\n      <td>It is primarily a farming community.</td>\n      <td>196992</td>\n      <td>86400</td>\n      <td>val</td>\n    </tr>\n    <tr>\n      <th>38728</th>\n      <td>64bb74229c8a05aeeebe8aeb498d5a0389479625a0085d...</td>\n      <td>common_voice_en_19761030.mp3</td>\n      <td>twenties</td>\n      <td>male</td>\n      <td>The Ewings had thirteen children.</td>\n      <td>171648</td>\n      <td>168960</td>\n      <td>val</td>\n    </tr>\n    <tr>\n      <th>38729</th>\n      <td>ac0b5abbc3c48b594b7e837a23e68460b0b0329c4df554...</td>\n      <td>common_voice_en_19683374.mp3</td>\n      <td>twenties</td>\n      <td>female</td>\n      <td>Under protest but at Ben-Gurion's insistence, ...</td>\n      <td>357120</td>\n      <td>293760</td>\n      <td>train</td>\n    </tr>\n  </tbody>\n</table>\n<p>38730 rows × 8 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "dl.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_TFRecord(writer: tf.io.TFRecordWriter, x_dict: dict, y_dict: dict) -> None:\n",
    "    \"\"\"\n",
    "    Append data to open writer.\n",
    "    :param writer: TFRecordWriter\n",
    "    :param x_dict: dict with np.arrays\n",
    "    :param y_dict: dict with np.arrays\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    def _bytes_feature(value):\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "    features = dict()\n",
    "    for key in x_dict.keys():\n",
    "        features[key] = _bytes_feature(tf.compat.as_bytes(x_dict[key].astype(np.float32).tostring()))\n",
    "    for key in y_dict.keys():\n",
    "        features[key] = _bytes_feature(tf.compat.as_bytes(y_dict[key].astype(np.float32).tostring()))\n",
    "    example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "    writer.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "path = os.path.join('data', 'tf_record')\n",
    "file_name = 'data_file_train.tfrecord'\n",
    "Path(path).mkdir(parents=True, exist_ok=True)\n",
    "options = tf.io.TFRecordOptions(compression_level=1, compression_type=\"ZLIB\")\n",
    "\n",
    "with tf.io.TFRecordWriter(os.path.join(path, file_name), options=options) as writer:\n",
    "    for spec_num in tqdm(np.arange(dl.train.shape[0]+1)):\n",
    "        spec = dl.make_spectrogram(spec_num)\n",
    "        paded_spec = dl.pad_spec(spec)\n",
    "        set_type = dl.train.iloc[spec_num, 7]\n",
    "        age = dl.train.iloc[spec_num, 2]\n",
    "        gender = dl.train.iloc[spec_num, 3]\n",
    "        for window in np.arange(0, paded_spec.shape[1]-64, 64):\n",
    "            spec_window = paded_spec[:, window:window + 128]\n",
    "            x_dict = {'x': np.array(spec_window)}\n",
    "            y_dict = {'y_age':np.array(age), 'y_gender':np.array(gender)}\n",
    "            append_to_TFRecord(writer=writer, x_dict=x_dict, y_dict=y_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pathlist = Path(os.path.join('data', 'images')).rglob('*.npy')\n",
    "# path_list = []\n",
    "# for path in pathlist:\n",
    "#     path_list.append(str(path))\n",
    "\n",
    "# df = pd.DataFrame({'path':path_list})\n",
    "# df['set_type'] = df['path'].apply(lambda string: os.path.normpath(string).split(os.sep)[2])\n",
    "# df['gender'] = df['path'].apply(lambda string: os.path.normpath(string).split(os.sep)[3])\n",
    "# df['age'] = df['path'].apply(lambda string: os.path.normpath(string).split(os.sep)[4])\n",
    "\n",
    "# df = df.sample(frac = 1).reset_index(drop=True)\n",
    "# df.to_csv(os.path.join('data', 'images', 'image_metadata.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 38782/38782 [44:11<00:00, 14.63it/s]\n"
     ]
    }
   ],
   "source": [
    "lengths_cut = []\n",
    "for spec_num in tqdm(np.arange(38781+1)):\n",
    "    audio = np.array(dl.load_audio_binary(spec_num))\n",
    "    cut_audio = dl.cut_voice(audio)\n",
    "    lengths_cut.append(len(cut_audio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.6941762549697098"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "np.sum(lengths_cut)/dl.train.length.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "male      3713\n",
       "female    2285\n",
       "Name: gender, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "train[train.age == 'teens'].gender.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "female    20005\n",
       "male      18777\n",
       "Name: gender, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "train.gender.value_counts( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io.wavfile import write\n",
    "\n",
    "scaled = np.int16(audio/np.max(np.abs(audio)) * 32767)\n",
    "write('testOG.wav', 48000, scaled)\n",
    "\n",
    "scaled2 = np.int16(cut_audio/np.max(np.abs(cut_audio)) * 32767)\n",
    "write('test_recovered.wav', 48000, scaled2)"
   ]
  }
 ]
}