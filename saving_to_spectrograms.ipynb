{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bittfgpuconda091551eaf16844e1bd1fb06b4d0d7b6d",
   "display_name": "Python 3.7.7 64-bit ('tf_gpu': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TRAIN_DATA_LEN: 3714569\nVAL_DATA_LEN: 1590319\n"
     ]
    }
   ],
   "source": [
    "import path_configs # noqa\n",
    "import tensorflow as tf\n",
    "import settings\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import numpy as np\n",
    "import librosa\n",
    "settings.init()\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=1.5, font=\"Arial\", style=\"white\")\n",
    "# from modules.ClassifierGenerators import (TrainClassifierGenerator,  # noqa\n",
    "#                                           ValidationClassifierGenerator)  # noqa\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assumes that array is not zero\n",
    "def scaled(tensor):\n",
    "    return (tensor-tf.math.reduce_min(tensor))/(tf.math.reduce_max(tensor)-tf.\n",
    "                                                math.reduce_min(tensor))\n",
    "def scaled_array(array):\n",
    "    return (array - np.min(array))/(np.max(array) - np.min(array))\n",
    "\n",
    "import scipy.spatial.distance as dist\n",
    "def similarity(x1, x2):\n",
    "    x1 = librosa.feature.mfcc(x1).flatten()\n",
    "    x2 = scipy.signal.resample(librosa.feature.mfcc(x2).flatten(), len(x1))\n",
    "    distance = dist.correlation(x1, x2)\n",
    "    return 1/(1+distance)\n",
    "\n",
    "from modules.DataPreprocessor import DataLoader\n",
    "dl = DataLoader()\n",
    "sample_rate = 48000\n",
    "window_time = dl.window_time\n",
    "frame_length = int(48000*window_time/1000)\n",
    "frame_step = frame_length//4\n",
    "def reverse_to_audio(db_spec):\n",
    "    audio_rev_spec = librosa.db_to_power(db_spec, ref=1.0)\n",
    "    print(\n",
    "        'reversing'\n",
    "    )\n",
    "    return librosa.feature.inverse.mel_to_audio(audio_rev_spec, sr=48000, n_fft=frame_length, hop_length=frame_length//4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# for set_name in ['test', 'train', 'val']:\n",
    "#     for gender_name in ['male', 'female']:\n",
    "#         for age_name in ['teens', 'twenties','seventies', 'fifties', 'fourties','thirties', 'sixties', 'eighties']:        \n",
    "#             Path(os.path.join('data', 'images',set_name, gender_name, age_name)).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# for spec_num in tqdm(np.arange(dl.train.shape[0]+1)):\n",
    "#     spec = dl.make_spectrogram(spec_num)\n",
    "#     paded_spec = dl.pad_spec(spec)\n",
    "#     set_type = dl.train.iloc[spec_num, 7]\n",
    "#     age = dl.train.iloc[spec_num, 2]\n",
    "#     gender = dl.train.iloc[spec_num, 3]\n",
    "#     for window in np.arange(0, paded_spec.shape[1]-64, 64):\n",
    "#         spec_window = paded_spec[:, window:window + 128]\n",
    "#         name = str('spec_' + str(window//64) + '_' + str(spec_num))\n",
    "#         np.save(os.path.join('data', 'images',set_type, gender, age, name), spec_window, allow_pickle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_TFRecord(writer: tf.io.TFRecordWriter, x_dict: dict, y_dict: dict) -> None:\n",
    "    \"\"\"\n",
    "    Append data to open writer.\n",
    "    :param writer: TFRecordWriter\n",
    "    :param x_dict: dict with np.arrays\n",
    "    :param y_dict: dict with np.arrays\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    def _bytes_feature(value):\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "    features = dict()\n",
    "    for key in x_dict.keys():\n",
    "        features[key] = _bytes_feature(tf.compat.as_bytes(x_dict[key].astype(np.float32).tostring()))\n",
    "    for key in y_dict.keys():\n",
    "        features[key] = _bytes_feature(tf.compat.as_bytes(y_dict[key].astype(np.float32).tostring()))\n",
    "    example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "    writer.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/30890 [00:00<?, ?it/s]\n",
      "Saving train set to: data\\tf_record\\data_train.tfrecord\n",
      "100%|██████████| 30890/30890 [1:04:55<00:00,  7.93it/s]\n",
      "  0%|          | 1/7744 [00:00<18:42,  6.90it/s]\n",
      "Saving val set to: data\\tf_record\\data_val.tfrecord\n",
      "100%|██████████| 7744/7744 [16:50<00:00,  7.67it/s]\n",
      "  1%|          | 1/96 [00:00<00:10,  8.69it/s]\n",
      "Saving test set to: data\\tf_record\\data_test.tfrecord\n",
      "100%|██████████| 96/96 [00:13<00:00,  7.29it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "path = os.path.join('data', 'tf_record')\n",
    "Path(path).mkdir(parents=True, exist_ok=True)\n",
    "options = tf.io.TFRecordOptions(compression_level=1, compression_type=\"ZLIB\")\n",
    "\n",
    "for set_type in dl.train.set_type.unique():\n",
    "    # setting set_type for tf.record file\n",
    "    print('\\nSaving', str(set_type), 'set to:', os.path.join(path, ('data_' + str(set_type) + '.tfrecord')))\n",
    "    train_df = dl.train[dl.train.set_type == set_type]\n",
    "    # mapping string labels to floats\n",
    "    train_df.age = train_df.age.map({'teens': 15/100, 'twenties': 25/100,\n",
    "                                        'seventies': 75/100, 'fifties': 55/100,\n",
    "                                        'fourties': 45/100, 'thirties': 35/100,\n",
    "                                        'sixties': 65/100, 'eighties': 85/100}\n",
    "                                        )\n",
    "    train_df.gender = train_df.gender.map({'male': 0, 'female': 1})\n",
    "\n",
    "    with tf.io.TFRecordWriter(os.path.join(path, ('data_' + str(set_type) + '.tfrecord')), options=options) as writer:\n",
    "        for spec_num in tqdm(train_df.index.to_numpy()):\n",
    "            # getting spectrograms and labels from data_loader\n",
    "            spec = dl.make_spectrogram(spec_num)\n",
    "            paded_spec = dl.pad_spec(spec)\n",
    "            age = train_df.loc[spec_num][2]\n",
    "            gender = train_df.loc[spec_num][3]\n",
    "            # windowing spectrograms\n",
    "            for window in np.arange(0, paded_spec.shape[1]-128, 128):\n",
    "                spec_window = paded_spec[:, window:window + 256]\n",
    "                # making dicts for tfrecord writer\n",
    "                mfcc = librosa.feature.mfcc(S=spec_window, n_mfcc=36)\n",
    "                x_dict = {'x': np.array(spec_window), 'x_mfcc': np.array(mfcc)}\n",
    "                y_dict = {'y_age':np.array(age), 'y_gender':np.array(gender)}\n",
    "                append_to_TFRecord(writer=writer, x_dict=x_dict, y_dict=y_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dict = {'x': [128, 128, 1]}\n",
    "y_dict = {'y_age':[1], 'y_gender':[1]}\n",
    "dset = read_TFRecord(x_dict, y_dict, 2, os.path.join('data', 'tf_record', 'data_train.tfrecord'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "32.2 ms ± 214 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "for X, y in dset.batch(64).take(1):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['data\\\\tf_record\\\\data_train.tfrecord']"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "tf.io.gfile.glob(os.path.join('data', 'tf_record', 'data_train.tfrecord'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pathlist = Path(os.path.join('data', 'images')).rglob('*.npy')\n",
    "# path_list = []\n",
    "# for path in pathlist:\n",
    "#     path_list.append(str(path))\n",
    "\n",
    "# df = pd.DataFrame({'path':path_list})\n",
    "# df['set_type'] = df['path'].apply(lambda string: os.path.normpath(string).split(os.sep)[2])\n",
    "# df['gender'] = df['path'].apply(lambda string: os.path.normpath(string).split(os.sep)[3])\n",
    "# df['age'] = df['path'].apply(lambda string: os.path.normpath(string).split(os.sep)[4])\n",
    "\n",
    "# df = df.sample(frac = 1).reset_index(drop=True)\n",
    "# df.to_csv(os.path.join('data', 'images', 'image_metadata.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 38782/38782 [44:11<00:00, 14.63it/s]\n"
     ]
    }
   ],
   "source": [
    "lengths_cut = []\n",
    "for spec_num in tqdm(np.arange(38781+1)):\n",
    "    audio = np.array(dl.load_audio_binary(spec_num))\n",
    "    cut_audio = dl.cut_voice(audio)\n",
    "    lengths_cut.append(len(cut_audio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.6941762549697098"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "np.sum(lengths_cut)/dl.train.length.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "male      3713\n",
       "female    2285\n",
       "Name: gender, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "train[train.age == 'teens'].gender.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "female    20005\n",
       "male      18777\n",
       "Name: gender, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "train.gender.value_counts( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io.wavfile import write\n",
    "\n",
    "scaled = np.int16(audio/np.max(np.abs(audio)) * 32767)\n",
    "write('testOG.wav', 48000, scaled)\n",
    "\n",
    "scaled2 = np.int16(cut_audio/np.max(np.abs(cut_audio)) * 32767)\n",
    "write('test_recovered.wav', 48000, scaled2)"
   ]
  }
 ]
}